{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"svm_classifier_practice_david.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"eNoX-SR7h6XY","colab_type":"text"},"cell_type":"markdown","source":["# Multiclass support vector machine\n","\n","In this exercise you are going to implement a multiclass support vector machine. You will implement:\n","\n","* Load and preprocess the dataset.\n","* Implement loss function.\n","* Implement gradient descent."]},{"metadata":{"id":"5-Bh5Xhoh6Xb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":333},"outputId":"3d49fc1e-035b-41de-ceba-647810bf8306","executionInfo":{"status":"error","timestamp":1543361783200,"user_tz":300,"elapsed":812,"user":{"displayName":"Vanessa Rodriguez","photoUrl":"https://lh6.googleusercontent.com/-zPtZDIVITjI/AAAAAAAAAAI/AAAAAAAAJbs/7GM9TqabuN0/s64/photo.jpg","userId":"09708172555593347366"}}},"cell_type":"code","source":["# import libraries\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import sys\n","import time\n","sys.path.insert(0, \"../tools/\")\n","from utils import load_cifar10"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-625a5ff8b59d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../tools/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_cifar10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"metadata":{"id":"IEI-f-APifri","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Im7zJhRMh6Xf","colab_type":"code","colab":{}},"cell_type":"code","source":["# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n","try:\n","    del X_train, y_train\n","    del X_test, y_test\n","    print('Clear previously loaded data.')\n","except:\n","    pass\n","\n","X_train, y_train, X_test, y_test = load_cifar10()\n","\n","# As a sanity check, we print out the size of the training and test data.\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', y_train.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cUcw55w7h6Xj","colab_type":"text"},"cell_type":"markdown","source":["## Data visualization"]},{"metadata":{"id":"enBMcRUkh6Xl","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","TODO: Visualize some examples from the dataset. Make a grid of images with matplotlib\n","      and try to make that each column belongs to a class\n","\"\"\"\n","plt.imshow(X_train[1000].astype('uint8'))\n","print(y_train[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pIN5QsF7h6Xo","colab_type":"code","colab":{}},"cell_type":"code","source":["# Split the data into train, val, and test sets. In addition we will\n","# create a small development set as a subset of the training data;\n","# we can use this for development so our code runs faster.\n","num_training = 5000\n","num_validation = 1000\n","num_test = 600\n","num_dev = 500\n","\n","# Our validation set will be num_validation points from the original\n","# training set.\n","mask = range(num_training, num_training + num_validation)\n","X_val = X_train[mask]\n","y_val = y_train[mask]\n","\n","# Our training set will be the first num_train points from the original\n","# training set.\n","mask = range(num_training)\n","X_train = X_train[mask]\n","y_train = y_train[mask]\n","\n","# We will also make a development set, which is a small subset of\n","# the training set.\n","mask = np.random.choice(num_training, num_dev, replace=False)\n","X_dev = X_train[mask]\n","y_dev = y_train[mask]\n","\n","# We use the first num_test points of the original test set as our\n","# test set.\n","mask = range(num_test)\n","X_test = X_test[mask]\n","y_test = y_test[mask]\n","\n","print('Train data shape: ', X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pUbYvZIzh6Xs","colab_type":"code","colab":{}},"cell_type":"code","source":["# Preprocessing: reshape the image data into rows\n","X_train = np.reshape(X_train, (X_train.shape[0], -1))\n","X_val = np.reshape(X_val, (X_val.shape[0], -1))\n","X_test = np.reshape(X_test, (X_test.shape[0], -1))\n","X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n","\n","# As a sanity check, print out the shapes of the data\n","print('Training data shape: ', X_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('dev data shape: ', X_dev.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"83r3mUM4h6Xw","colab_type":"code","colab":{}},"cell_type":"code","source":["# Preprocessing: subtract the mean image\n","# Compute the image mean based on the training data\n","mean_image = np.mean(X_train, axis=0)\n","\n","# print a few of the elements\n","print(mean_image[:10])\n","\n","# Visualize the mean image using matplotlib                             \n","plt.figure(figsize=(4,4))\n","plt.imshow(mean_image.reshape((32,32,3)).astype('uint8'))\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_2g2LCEwh6Xz","colab_type":"code","colab":{}},"cell_type":"code","source":["# Subtract the mean to the training and testing set                     \n","X_train -= mean_image\n","X_val -= mean_image\n","X_test -= mean_image\n","X_dev -= mean_image"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VufjiKqVh6X2","colab_type":"code","colab":{}},"cell_type":"code","source":["# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n","# only has to worry about optimizing a single weight matrix W.\n","X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n","X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n","X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n","X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n","\n","print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U0uQvAw5h6X7","colab_type":"code","colab":{}},"cell_type":"code","source":["def svm_loss_loop(W, X, y, reg, delta=1):\n","    \"\"\"\n","    Structured SVM loss function, implementation with loops \n","\n","    D: number of features at the input vector\n","    C: number of classes in the dataset\n","    N: number of samples to operate on minibatches\n","\n","    Inputs\n","     W: A numpy array of shape (D, C) containing weights.\n","     X: A numpy array of shape (N, D) containing a minibatch of data.\n","     y: A numpy array of shape (N,) containing training labels; y[i] = c means\n","    that X[i] has label c, where 0 <= c < C.\n","     reg: (float) regularization strength\n","\n","    output:\n","     loss: the total loss as single float\n","     dW: gradients with respect to weights W; an array of same shape as W\n","    \"\"\"\n","    dW = np.zeros(W.shape) # initialize the gradient as zero\n","\n","    # compute the loss and the gradient\n","    num_classes = W.shape[1]\n","    num_train = X.shape[0]\n","    loss = 0.0\n","    for i in range(num_train):\n","        scores = X[i].dot(W)\n","        correct_class_score = scores[y[i]]\n","        diff_count = 0\n","        for j in range(num_classes):\n","            if j == y[i]:\n","                continue\n","            margin = scores[j] - correct_class_score + delta\n","            if margin > 0:\n","                loss += margin\n","                dW[:,j] += X[i]\n","                dW[:,y[i]] += -1*X[i]\n","\n","\n","    loss /= num_train\n","    dW /= num_train\n","    \n","    loss += reg*np.sum(W*W)\n","    dW += 2*reg*W\n","    # TODO: compute loss and gradients for this function, take into account \n","    # regularization in the loss and the gradients\n","    return loss, dW"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0bokOgonh6X-","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"pwDb4sdYh6YB","colab_type":"code","colab":{}},"cell_type":"code","source":["# generate a random SVM weight matrix of small numbers\n","W = np.random.randn(3073, 10) * 0.0001 \n","\n","loss, grad = svm_loss_loop(W, X_dev, y_dev, 0)\n","print('loss: {}'.format(loss))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cjZA1f9Wh6YF","colab_type":"code","colab":{}},"cell_type":"code","source":["# Numerically compute the gradient along several randomly chosen dimensions, and\n","# compare them with your analytically computed gradient. The numbers should match\n","# almost exactly along all dimensions.\n","loss, grad = svm_loss_loop(W, X_dev, y_dev, 0)\n","\n","from gradient_check import grad_check_sparse\n","f = lambda w: svm_loss_loop(w, X_dev, y_dev, 0.0)[0]\n","grad_numerical = grad_check_sparse(f, W, grad)\n","\n","# do the gradient check once again with regularization turned on\n","# you didn't forget the regularization gradient did you?\n","loss, grad = svm_loss_loop(W, X_dev, y_dev, 5e1)\n","f = lambda w: svm_loss_loop(w, X_dev, y_dev, 5e1)[0]\n","grad_numerical = grad_check_sparse(f, W, grad)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vFR-ABOMh6YI","colab_type":"code","colab":{}},"cell_type":"code","source":["def svm_loss_vectorization(W, X, y, reg, delta=1):\n","    \"\"\"\n","    Structured SVM loss function, implementation with vectorization\n","\n","    D: number of features at the input vector\n","    C: number of classes in the dataset\n","    N: number of samples to operate on minibatches\n","\n","    Inputs\n","     W: A numpy array of shape (D, C) containing weights.\n","     X: A numpy array of shape (N, D) containing a minibatch of data.\n","     y: A numpy array of shape (N,) containing training labels; y[i] = c means\n","    that X[i] has label c, where 0 <= c < C.\n","     reg: (float) regularization strength\n","\n","    Output:\n","     loss: the total loss as single float\n","     dW: gradients with respect to weights W; an array of same shape as W\n","    \"\"\"\n","    loss = 0\n","    dW = np.zeros(W.shape)\n","    \n","    ##########################################################################\n","    # TODO: compute loss and gradients for this function, take into account  #\n","    # regularization in the loss and the gradients                           #\n","    # START                                                                  #\n","    ##########################################################################\n","    \n","    # compute loss\n","    num_train = X.shape[0]\n","    num_classes = np.max(y) + 1\n","    dot_xw = np.dot(X, W) # (N x C)\n","\n","    correct_scores = dot_xw[np.arange(num_train), y].reshape(-1, 1)\n","\n","    margins = dot_xw - correct_scores\n","    margins += delta\n","    margins[np.arange(num_train), y] = 0\n","    id_margins_greater_than_0 = margins > 0\n","    margins_to_sum = margins * id_margins_greater_than_0\n","    #margins_to_sum[np.arange(num_train), y] = 0\n","\n","    loss = (1/num_train)*np.sum(margins_to_sum) + reg*np.sum(W*W)\n","    \n","    # compute gradients\n","    xmat = np.zeros(dot_xw.shape)\n","    xmat[id_margins_greater_than_0] = 1\n","    num_incorrect = -1 * np.sum(xmat, axis=1)\n","    xmat[np.arange(num_train), y] = num_incorrect\n","    dW = np.dot(X.T, xmat)\n","    dW /= num_train\n","    dW += 2*reg*W\n","    ##########################################################################\n","    # END                                                                    #\n","    ##########################################################################\n","    return loss, dW"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nGKFJWuhh6YK","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"b3toMpBEh6YM","colab_type":"code","colab":{}},"cell_type":"code","source":["# Complete the implementation of svm_loss_vectorization, and compute the gradient\n","# of the loss function in a vectorized way.\n","\n","# The naive implementation and the vectorized implementation should match, but\n","# the vectorized version should still be much faster.\n","tic = time.time()\n","_, grad_naive = svm_loss_loop(W, X_train, y_train, 0.00001)\n","toc = time.time()\n","print('Naive loss and gradient: computed in {}'.format(toc - tic))\n","\n","tic = time.time()\n","_, grad_vectorized = svm_loss_vectorization(W, X_train, y_train, 0.00001)\n","toc = time.time()\n","print('vectorized loss and gradient: computed in {}'.format(toc - tic))\n","\n","# The loss is a single number, so it is easy to compare the values computed\n","# by the two implementations. The gradient on the other hand is a matrix, so\n","# we use the Frobenius norm to compare them.\n","difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n","print('difference: {}'.format(difference))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4aDXJOo1h6YP","colab_type":"code","colab":{}},"cell_type":"code","source":["# TODO: Implement the svm classifier class using the loss previous function and plot the loss for the svm classifier\n","class customSVMClassifier(object):\n","    def __init__(self, X, Y, x_val=None, y_val=None,lr=1e-1, reg=1e-5, delta=1, num_iter=800, batch_size=100):\n","        self.x_train = X\n","        self.y_train = Y\n","        self.x_val = x_val\n","        self.y_val = y_val\n","        self.lr = lr\n","        self.reg = reg\n","        self.num_iter = num_iter\n","        self.batch_size = batch_size\n","        self.loss_history = list()\n","        self.loss_val = list()\n","        self.delta = delta\n","        self.W = np.random.randn(X.shape[1], np.max(Y) + 1) * 0.01\n","    def svm_loss_vectorization(self, W, X, y, reg, delta):\n","        \"\"\"\n","        Structured SVM loss function, implementation with vectorization\n","\n","        D: number of features at the input vector\n","        C: number of classes in the dataset\n","        N: number of samples to operate on minibatches\n","\n","        Inputs\n","         W: A numpy array of shape (D, C) containing weights.\n","         X: A numpy array of shape (N, D) containing a minibatch of data.\n","         y: A numpy array of shape (N,) containing training labels; y[i] = c means\n","        that X[i] has label c, where 0 <= c < C.\n","         reg: (float) regularization strength\n","\n","        Output:\n","         loss: the total loss as single float\n","         dW: gradients with respect to weights W; an array of same shape as W\n","        \"\"\"\n","        loss = 0\n","        dW = np.zeros(W.shape)\n","\n","        ##########################################################################\n","        # TODO: compute loss and gradients for this function, take into account  #\n","        # regularization in the loss and the gradients                           #\n","        # START                                                                  #\n","        ##########################################################################\n","\n","        # compute loss\n","        # compute loss\n","        num_train = X.shape[0]\n","        num_classes = np.max(y) + 1\n","        dot_xw = np.dot(X, W) # (N x C)\n","\n","        correct_scores = dot_xw[np.arange(num_train), y].reshape(-1, 1)\n","\n","        margins = dot_xw - correct_scores\n","        margins += delta\n","        margins[np.arange(num_train), y] = 0\n","        id_margins_greater_than_0 = margins > 0\n","        margins_to_sum = margins * id_margins_greater_than_0\n","        #margins_to_sum[np.arange(num_train), y] = 0\n","\n","        loss = (1/num_train)*np.sum(margins_to_sum) + reg*np.sum(W*W)\n","\n","        # compute gradients\n","        xmat = np.zeros(dot_xw.shape)\n","        xmat[id_margins_greater_than_0] = 1\n","        num_incorrect = -1 * np.sum(xmat, axis=1)\n","        xmat[np.arange(num_train), y] = num_incorrect\n","        dW = np.dot(X.T, xmat)\n","        dW /= num_train\n","        dW += 2*reg*W\n","        ##########################################################################\n","        # END                                                                    #\n","        ##########################################################################\n","        return loss, dW\n","    def train(self):\n","        num_train = self.x_train.shape[0]\n","        for k in range(self.num_iter):\n","            X_batch = None\n","            y_batch = None\n","\n","            #########################################################################\n","            # TODO:                                                                 #\n","            # Implement SGD (stochastic gradient descent)                           #\n","            # Hint: look for the function in numpy np.random.choice                 #\n","            #########################################################################\n","            # randomly sample some samples\n","            sample_indices = np.random.choice(np.arange(num_train), self.batch_size)\n","            x_batch = self.x_train[sample_indices]\n","            y_batch = self.y_train[sample_indices]\n","\n","            # evaluate loss and gradient\n","            loss, grad = self.svm_loss_vectorization(self.W, x_batch, y_batch, self.reg, self.delta)\n","            self.loss_history.append(loss)\n","\n","            # perform parameter update\n","            self.W = self.W - self.lr * grad\n","            #########################################################################\n","            #  END                                                                  #               \n","            #########################################################################\n","    def predict(self, X):\n","        # TODO: Implement prediction function for svm\n","        #   X: input array with shape (N_test x D)\n","        return np.argmax(np.dot(X, self.W), axis=1)\n","    def get_loss(self):\n","        return self.loss_history\n","    def getW(self):\n","        return self.W"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IcTQJCRIh6YR","colab_type":"code","colab":{}},"cell_type":"code","source":["# Train a svm model and plot the loss\n","svm = customSVMClassifier(X_train, y_train, lr=1e-5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KSBrxrOoh6YX","colab_type":"code","colab":{}},"cell_type":"code","source":["svm.train()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nkCgY6Cgh6Ya","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.plot(svm.get_loss())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3pGEVokYh6Yf","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"w_gKtpGWh6Yh","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"TNypTEZhh6Yk","colab_type":"code","colab":{}},"cell_type":"code","source":["# Write the LinearSVM.predict function and evaluate the performance on both the\n","# training and validation set\n","y_train_pred = svm.predict(X_train)\n","print('training accuracy: {}'.format(np.mean(y_train == y_train_pred)))\n","y_val_pred = svm.predict(X_val)\n","print('validation accuracy: {}'.format(np.mean(y_val == y_val_pred)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-R2k5YDdh6Yn","colab_type":"code","colab":{}},"cell_type":"code","source":["# Use the validation set to tune hyperparameters (regularization strength and\n","# learning rate). You should experiment with different ranges for the learning\n","# rates and regularization strengths; if you are careful you should be able to\n","# get a classification accuracy of about 0.4 on the validation set.\n","\n","learning_rates = [1e-7, 2e-7, 5e-7, 1e-6]\n","regularization_strengths = [1e4, 2e4, 5e4, 1e5, 5e5, 1e6]\n","\n","# results is dictionary mapping tuples of the form\n","# (learning_rate, regularization_strength) to tuples of the form\n","# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n","# of data points that are correctly classified.\n","results = {}\n","best_val = -1   # The highest validation accuracy that we have seen so far.\n","best_svm = None # The LinearSVM object that achieved the highest validation rate.\n","\n","################################################################################\n","# TODO:                                                                        #\n","# Write code that chooses the best hyperparameters by tuning on the validation #\n","# set. For each combination of hyperparameters, train a linear SVM on the      #\n","# training set, compute its accuracy on the training and validation sets, and  #\n","# store these numbers in the results dictionary. In addition, store the best   #\n","# validation accuracy in best_val and the LinearSVM object that achieves this  #\n","# accuracy in best_svm.                                                        #\n","#                                                                              #\n","# Hint: You should use a small value for num_iters as you develop your         #\n","# validation code so that the SVMs don't take much time to train; once you are #\n","# confident that your validation code works, you should rerun the validation   #\n","# code with a larger value for num_iters.                                      #\n","# START                                                                        #\n","################################################################################\n","for learning in learning_rates:\n","    for regularization in regularization_strengths:\n","        pass\n","################################################################################\n","# END                                                                          #                                \n","################################################################################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xTo7jzigh6Yp","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"lSKxcJ4Bh6Yr","colab_type":"code","colab":{}},"cell_type":"code","source":["# Visualize the learned weights for each class.\n","# Depending on your choice of learning rate and regularization strength, these may\n","# or may not be nice to look at.\n","w = best_svm.W[:-1,:] # strip out the bias\n","w = w.reshape(10, 32, 32, 3)\n","w_min, w_max = np.min(w), np.max(w)\n","classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","for i in range(10):\n","    plt.subplot(2, 5, i + 1)\n","    \n","    # Rescale the weights to be between 0 and 255\n","    wimg = 255.0 * (w[i].squeeze() - w_min) / (w_max - w_min)\n","    plt.imshow(wimg.astype('uint8'))\n","    plt.axis('off')\n","    plt.title(classes[i])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YhQBTiq1h6Yt","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"DhEno7Ayh6Yv","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}