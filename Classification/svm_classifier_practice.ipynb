{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass support vector machine\n",
    "\n",
    "In this exercise you are going to implement a multiclass support vector machine. You will implement:\n",
    "\n",
    "* Load and preprocess the dataset.\n",
    "* Implement loss function.\n",
    "* Implement gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "sys.path.insert(0, \"../tools/\")\n",
    "from utils import load_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_cifar10()\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Visualize some examples from the dataset. Make a grid of images with matplotlib\n",
    "      and try to make that each column belongs to a class\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, val, and test sets. In addition we will\n",
    "# create a small development set as a subset of the training data;\n",
    "# we can use this for development so our code runs faster.\n",
    "num_training = 5000\n",
    "num_validation = 1000\n",
    "num_test = 600\n",
    "num_dev = 500\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "# As a sanity check, print out the shapes of the data\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: subtract the mean image\n",
    "# Compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "\n",
    "# print a few of the elements\n",
    "print(mean_image[:10])\n",
    "\n",
    "# Visualize the mean image using matplotlib                             \n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the mean to the training and testing set                     \n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_loop(W, X, y, reg, delta=1):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, implementation with loops \n",
    "\n",
    "    D: number of features at the input vector\n",
    "    C: number of classes in the dataset\n",
    "    N: number of samples to operate on minibatches\n",
    "\n",
    "    Inputs\n",
    "     W: A numpy array of shape (D, C) containing weights.\n",
    "     X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "     y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "     reg: (float) regularization strength\n",
    "\n",
    "    output:\n",
    "     loss: the total loss as single float\n",
    "     dW: gradients with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    dW = np.zeros(W.shape) # initialize the gradient as zero\n",
    "\n",
    "    # compute the loss and the gradient\n",
    "    num_classes = W.shape[1]\n",
    "    num_train = X.shape[0]\n",
    "    loss = 0.0\n",
    "    for i in range(num_train):\n",
    "        scores = X[i].dot(W)\n",
    "        correct_class_score = scores[y[i]]\n",
    "        diff_count = 0\n",
    "        for j in range(num_classes):\n",
    "            if j == y[i]:\n",
    "                continue\n",
    "            if margin > 0:\n",
    "                loss = 0\n",
    "\n",
    "    \n",
    "    # TODO: compute loss and gradients for this function, take into account \n",
    "    # regularization in the loss and the gradients\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random SVM weight matrix of small numbers\n",
    "W = np.random.randn(3073, 10) * 0.0001 \n",
    "\n",
    "loss, grad = svm_loss_loop(W, X_dev, y_dev, 0)\n",
    "print('loss: {}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
    "# compare them with your analytically computed gradient. The numbers should match\n",
    "# almost exactly along all dimensions.\n",
    "loss, grad = svm_loss_loop(W, X_dev, y_dev, 0)\n",
    "\n",
    "from gradient_check import grad_check_sparse\n",
    "f = lambda w: svm_loss_loop(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)\n",
    "\n",
    "# do the gradient check once again with regularization turned on\n",
    "# you didn't forget the regularization gradient did you?\n",
    "loss, grad = svm_loss_loop(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: svm_loss_loop(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_loss_vectorization(W, X, y, reg, delta=1):\n",
    "    \"\"\"\n",
    "    Structured SVM loss function, implementation with vectorization\n",
    "\n",
    "    D: number of features at the input vector\n",
    "    C: number of classes in the dataset\n",
    "    N: number of samples to operate on minibatches\n",
    "\n",
    "    Inputs\n",
    "     W: A numpy array of shape (D, C) containing weights.\n",
    "     X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "     y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "     reg: (float) regularization strength\n",
    "\n",
    "    Output:\n",
    "     loss: the total loss as single float\n",
    "     dW: gradients with respect to weights W; an array of same shape as W\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    dW = np.zeros(W.shape)\n",
    "    \n",
    "    ##########################################################################\n",
    "    # TODO: compute loss and gradients for this function, take into account  #\n",
    "    # regularization in the loss and the gradients                           #\n",
    "    # START                                                                  #\n",
    "    ##########################################################################\n",
    "    \n",
    "    # compute loss\n",
    "    \n",
    "    ##########################################################################\n",
    "    # END                                                                    #\n",
    "    ##########################################################################\n",
    "    return loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the implementation of svm_loss_vectorization, and compute the gradient\n",
    "# of the loss function in a vectorized way.\n",
    "\n",
    "# The naive implementation and the vectorized implementation should match, but\n",
    "# the vectorized version should still be much faster.\n",
    "tic = time.time()\n",
    "_, grad_naive = svm_loss_loop(W, X_train, y_train, 0.00001)\n",
    "toc = time.time()\n",
    "print('Naive loss and gradient: computed in {}'.format(toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = svm_loss_vectorization(W, X_train, y_train, 0.00001)\n",
    "toc = time.time()\n",
    "print('vectorized loss and gradient: computed in {}'.format(toc - tic))\n",
    "\n",
    "# The loss is a single number, so it is easy to compare the values computed\n",
    "# by the two implementations. The gradient on the other hand is a matrix, so\n",
    "# we use the Frobenius norm to compare them.\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('difference: {}'.format(difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the svm classifier class using the loss previous function and plot the loss for the svm classifier\n",
    "class customSVMClassifier(object):\n",
    "    def __init__(self, X, Y, lr=1e-1, reg=1e-5, delta=1, num_iter=800, batch_size=100):\n",
    "        self.x_train = X\n",
    "        self.y_train = Y\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        self.num_iter = num_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_history = list()\n",
    "        self.delta = delta\n",
    "        self.W = np.random.randn(X.shape[1], np.max(Y) + 1) * 0.01\n",
    "    def svm_loss_vectorization(self, W, X, y, reg, delta):\n",
    "        \"\"\"\n",
    "        Structured SVM loss function, implementation with vectorization\n",
    "\n",
    "        D: number of features at the input vector\n",
    "        C: number of classes in the dataset\n",
    "        N: number of samples to operate on minibatches\n",
    "\n",
    "        Inputs\n",
    "         W: A numpy array of shape (D, C) containing weights.\n",
    "         X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "         y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "        that X[i] has label c, where 0 <= c < C.\n",
    "         reg: (float) regularization strength\n",
    "\n",
    "        Output:\n",
    "         loss: the total loss as single float\n",
    "         dW: gradients with respect to weights W; an array of same shape as W\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        dW = np.zeros(W.shape)\n",
    "\n",
    "        ##########################################################################\n",
    "        # TODO: compute loss and gradients for this function, take into account  #\n",
    "        # regularization in the loss and the gradients                           #\n",
    "        # START                                                                  #\n",
    "        ##########################################################################\n",
    "\n",
    "        \n",
    "        ##########################################################################\n",
    "        # END                                                                    #\n",
    "        ##########################################################################\n",
    "        return loss, dW\n",
    "    def train(self):\n",
    "        num_train = self.x_train.shape[0]\n",
    "        for k in range(self.num_iter):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Implement SGD (stochastic gradient descent)                           #\n",
    "            # Hint: look for the function in numpy np.random.choice                 #\n",
    "            #########################################################################\n",
    "            # randomly sample some samples\n",
    "            \n",
    "            #########################################################################\n",
    "            #  END                                                                  #               \n",
    "            #########################################################################\n",
    "    def predict(self, X):\n",
    "        # TODO: Implement prediction function for svm\n",
    "        #   X: input array with shape (N_test x D)\n",
    "        pass\n",
    "    def get_loss(self):\n",
    "        return self.loss_history\n",
    "    def getW(self):\n",
    "        return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a svm model and plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the LinearSVM.predict function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = svm.predict(X_train)\n",
    "print('training accuracy: {}'.format(np.mean(y_train == y_train_pred)))\n",
    "y_val_pred = svm.predict(X_val)\n",
    "print('validation accuracy: {}'.format(np.mean(y_val == y_val_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of about 0.4 on the validation set.\n",
    "\n",
    "learning_rates = [1e-7, 2e-7, 5e-7, 1e-6]\n",
    "regularization_strengths = [1e4, 2e4, 5e4, 1e5, 5e5, 1e6]\n",
    "\n",
    "# results is dictionary mapping tuples of the form\n",
    "# (learning_rate, regularization_strength) to tuples of the form\n",
    "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
    "# of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_svm = None # The LinearSVM object that achieved the highest validation rate.\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a linear SVM on the      #\n",
    "# training set, compute its accuracy on the training and validation sets, and  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the LinearSVM object that achieves this  #\n",
    "# accuracy in best_svm.                                                        #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the SVMs don't take much time to train; once you are #\n",
    "# confident that your validation code works, you should rerun the validation   #\n",
    "# code with a larger value for num_iters.                                      #\n",
    "################################################################################\n",
    "\n",
    "for learning in learning_rates:\n",
    "    for regularization in regularization_strengths:\n",
    "        pass\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr {0} reg {1} train accuracy: {2} val accuracy: {3}'.format(lr, reg, train_accuracy, val_accuracy))\n",
    "print('best validation accuracy achieved during cross-validation: {}'.format(best_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class.\n",
    "# Depending on your choice of learning rate and regularization strength, these may\n",
    "# or may not be nice to look at.\n",
    "w = best_svm.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(10, 32, 32, 3)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
